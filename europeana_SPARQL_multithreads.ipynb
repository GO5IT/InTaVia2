{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx_ClJU0CNOL"
      },
      "source": [
        "# Python scripts to generate metadata for Europeana objects which are created by persons (dc:creator, DBpedia URIs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAOP_vYOFhiz"
      },
      "outputs": [],
      "source": [
        "%pip install SPARQLWrapper\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install fsspec\n",
        "%pip install rdfpandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c7SpK0wWTjTP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON, XML, TURTLE, N3, RDF, RDFXML, CSV, TSV, JSONLD, DIGEST\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import requests\n",
        "from urllib import parse, error\n",
        "import fsspec\n",
        "from rdfpandas.graph import to_graph, to_dataframe\n",
        "from rdflib import Graph, URIRef, Literal, BNode, Namespace\n",
        "from rdflib.namespace import NamespaceManager,CSVW, DC, DCAT, DCTERMS, DOAP, FOAF, ODRL2, ORG, OWL, PROF, PROV, RDF, RDFS, SDO, SH, SKOS, SOSA, SSN, TIME, VOID, XMLNS, XSD\n",
        "\n",
        "# For multithreading\n",
        "#import concurrent.futures\n",
        "from concurrent import futures\n",
        "import threading\n",
        "# For time recording\n",
        "import time\n",
        "import datetime\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Xg-3YoMWbe",
        "outputId": "75834690-7c81-422e-aa2a-c6be119c867e"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "#from google.colab import drive, files\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Specify file location (Google Colab and local)\n",
        "filelocation_google = '/content/drive/MyDrive/Colab Notebooks/InTaVia/'\n",
        "filelocation_local = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOnM6yOt0VXE",
        "outputId": "d0776cdc-4dd1-4b62-ae6f-420427577fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Architecture:            arm64\n",
            "Byte Order:              Little Endian\n",
            "Total CPU(s):            8\n",
            "Model name:              MacBookPro18,3\n"
          ]
        }
      ],
      "source": [
        "!lscpu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BWiTllO3_N81"
      },
      "source": [
        "## Standalone Test for 2nd query for 1 record (Method 4: shortcut version without SPARQL i.e. HTTP request without RDFlib)\n",
        "API does not return the same data as SPARQL (e.g. DBpedia link is gone, Europeana Agent node is returned etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apnj4hHNxvsb"
      },
      "outputs": [],
      "source": [
        "# Specify any Europeana URIs (i.e. http://data.europeana.eu/item/ or http://data.europeana.eu/proxy/europeana/)\n",
        "europeanaURI = 'http://data.europeana.eu/item/2023859/_http___keptar_oszk_hu_025900_025984__'\n",
        "europeanaURI = 'http://data.europeana.eu/item/15508/3710'\n",
        "#europeanaURI = 'http://data.europeana.eu/proxy/europeana/2020903/KMS1811'\n",
        "#europeanaURI = 'http://data.europeana.eu/proxy/provider/2032004/20270'\n",
        "#wikidataURI = 'http://www.wikidata.org/entity/Q699736'\n",
        "#dbpediaURI = 'http://dbpedia.org/resource/Zac_Posen'\n",
        "#europeanaURI = 'http://data.europeana.eu/proxy/europeana/2026116/Partage_Plus_ProvidedCHO_Bildarchiv_Foto_Marburg_obj_20184057_LA_5_957_15a'\n",
        "#dbpediaURI = 'http://dbpedia.org/resource/Gustav_Klimt'\n",
        "headers = {\n",
        "    'Accept': 'text/turtle',\n",
        "    'Content-type': 'text/turtle'\n",
        "}\n",
        "\n",
        "def europeana_http_request(europeanaURI, headers):\n",
        "\n",
        "  # try 3 times\n",
        "  #r = requests.get(europeanaURI, headers=headers, timeout=3)\n",
        "  # try:\n",
        "  #   print(r.raise_for_status())\n",
        "  #   return(r.text)\n",
        "  # except requests.exceptions.HTTPError as e: \n",
        "  #   print(e)\n",
        "  #   return\n",
        "\n",
        "  # Try 3 times. In case of error, print it with one of the 4 types\n",
        "  try:\n",
        "    r = requests.get(europeanaURI, headers=headers, timeout=3)\n",
        "    print(r.raise_for_status())\n",
        "    return(r.text)\n",
        "  except requests.exceptions.HTTPError as error_h:\n",
        "      print(\"Http Error: \", error_h)\n",
        "      #return error_h\n",
        "  except requests.exceptions.ConnectionError as error_c:\n",
        "      print(\"Connection Error: \", error_c)\n",
        "      #return error_c\n",
        "  except requests.exceptions.Timeout as error_t:\n",
        "      print(\"Timeout Error: \", error_t)\n",
        "      #return error_t\n",
        "  except requests.exceptions.RequestException as error_o:\n",
        "      print(\"All Other Error: \", error_o)\n",
        "      #return error_o\n",
        "      return\n",
        "turtle = europeana_http_request(europeanaURI, headers)\n",
        "print(turtle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iShl79zfAj96"
      },
      "source": [
        "**Create a list of files in the CSV folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIAwp8atAj96",
        "outputId": "a8ac6713-1678-4f5d-936b-2bd561a56dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_0.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_10000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_100000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_110000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_120000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_130000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_140000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_150000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_160000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_170000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_180000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_190000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_20000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_200000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_210000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_220000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_230000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_240000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_250000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_260000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_30000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_40000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_50000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_60000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_70000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_80000.csv', 'OnlyInTaVia_at_nl_fi_si_URI_list_df_WikidataContributor_90000.csv']\n"
          ]
        }
      ],
      "source": [
        "# folder path to CSV files\n",
        "dir_path = 'EuropeanaObjectURIlist/Contributor'\n",
        "# Create an empty list\n",
        "list_file = []\n",
        "# Iterate in the folder\n",
        "for path in os.listdir(dir_path):\n",
        "    # check if current path is a file\n",
        "    if os.path.isfile(os.path.join(dir_path, path)):\n",
        "        list_file.append(path)\n",
        "list_file.sort()\n",
        "print(list_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create log file for Python console output\n",
        "\n",
        "# import sys\n",
        "# import logging\n",
        "\n",
        "# logging.basicConfig(\n",
        "#     format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
        "#     level=logging.INFO,\n",
        "#     datefmt='%Y-%m-%d %H:%M:%S',\n",
        "#     #stream=sys.stdout,\n",
        "#     filename=\"notebook.log\",\n",
        "# \tfilemode='w') \n",
        "# log = logging.getLogger('notebook')\n",
        "\n",
        "\n",
        "# logger = logging.getLogger()\n",
        "# fhandler = logging.FileHandler(filename='test_log.log', mode='a')\n",
        "# logger.addHandler(fhandler)\n",
        "# logging.warning('This is a warning message')\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(filename='logs.log', level=logging.INFO)\n",
        "logging.info(list_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWOj8Kvk0mVa"
      },
      "source": [
        "## Multi-threading "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChYKUCvwTzxa"
      },
      "source": [
        "**Multi-threading testing for 8 image download**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk9Xvd6e0k12"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "#import concurrent.futures\n",
        "from concurrent import futures\n",
        "import threading\n",
        "\n",
        "img_urls = [\n",
        "    'https://images.unsplash.com/photo-1516117172878-fd2c41f4a759',\n",
        "    'https://images.unsplash.com/photo-1524429656589-6633a470097c',\n",
        "    'https://images.unsplash.com/photo-1516972810927-80185027ca84',\n",
        "    'https://images.unsplash.com/photo-1516117172878-fd2c41f4a759',\n",
        "    'https://images.unsplash.com/photo-1550439062-609e1531270e',\n",
        "    'https://images.unsplash.com/photo-1532009324734-20a7a5813719',\n",
        "    'https://images.unsplash.com/photo-1516972810927-80185027ca84',\n",
        "    'https://images.unsplash.com/photo-1550439062-609e1531270e',\n",
        "    'https://images.unsplash.com/photo-1516117172878-fd2c41f4a759',\n",
        "    'https://images.unsplash.com/photo-1516972810927-80185027ca84',\n",
        "]\n",
        "\n",
        "def download_image(img_url):\n",
        "    img_bytes = requests.get(img_url).content\n",
        "    img_name = img_url.split('/')[3]\n",
        "    img_name = f'{img_name}.jpg'\n",
        "    with open(img_name, 'wb') as img_file:\n",
        "        img_file.write(img_bytes)\n",
        "        print(f'{img_name} was downloaded...')\n",
        "\n",
        "t1 = time.perf_counter()\n",
        "now1 = datetime.datetime.now()\n",
        "#Initiate the threads\n",
        "ex = futures.ThreadPoolExecutor(max_workers=35)\n",
        "print('{}: is starting work'.format(threading.current_thread().getName()))\n",
        "#Start the threads with the map method\n",
        "results = ex.map(download_image, img_urls)\n",
        "#print('{}: is waiting for the results'.format(threading.current_thread().getName()))\n",
        "real_results = list(results)\n",
        "print('main: results: {}'.format(real_results))\n",
        "t2 = time.perf_counter()\n",
        "now2 = datetime.datetime.now()\n",
        "duration_seconds = round(t2 - t1, 3)\n",
        "duration = str(timedelta(seconds=duration_seconds))\n",
        "n = '\\n'\n",
        "print(f'THREADING Finished in {duration_seconds} seconds')\n",
        "print(f'THREADING {n} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {n} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {n} Duration: {duration} = {duration_seconds} seconds')\n",
        "# Write ('a': Add text. I.e. preserve existing texts)\n",
        "with open('log_threading.txt', 'a') as f:\n",
        "    f.write(f'THREADING: {n} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {n} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {n} Duration: {duration} = {duration_seconds} seconds')\n",
        "\n",
        "#Compare it to a serial run\n",
        "t1 = time.perf_counter()\n",
        "now1 = datetime.datetime.now()\n",
        "for img_url in img_urls:\n",
        "    download_image(img_url)\n",
        "t2 = time.perf_counter()\n",
        "now2 = datetime.datetime.now()\n",
        "duration_seconds = round(t2 - t1, 3)\n",
        "duration = str(timedelta(seconds=duration_seconds))\n",
        "\n",
        "print(f'SERIAL Finished in {duration_seconds} seconds')\n",
        "print(f'SERIAL {n} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {n} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {n} Duration: {duration} = {duration_seconds} seconds')\n",
        "# Write ('a': Add text. I.e. preserve existing texts)\n",
        "with open('log_serial.txt', 'a') as f:\n",
        "    f.write(f'SERIAL {n} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {n} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {n} Duration: {duration} = {duration_seconds} seconds')\n",
        "\n",
        "# Output result:\n",
        "#THREADING: Finished in 1.9588102889999846 seconds\n",
        "#SERIAL: Finished in 2.8489685149999104 seconds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup to save a log file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqhMl-qQAj97"
      },
      "source": [
        "**MULTI-THREADING New way with HTTP request: 2nd Queries to fetch CH object metadata (Caution long processing time!)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h8XizMnUh9j",
        "outputId": "c0421b50-fa16-4888-e90b-99d2c6b70255"
      },
      "outputs": [],
      "source": [
        "# Test with 1 iteration\n",
        "#n4 = np.array([770000])\n",
        "#n4 = np.array(list_file[-2:])\n",
        "#print(n4)\n",
        "#list_file = list_file[1:4]\n",
        "print(list_file)\n",
        "\n",
        "def multiple_files_download(list_file):\n",
        "  i = 0\n",
        "  for n in list_file:\n",
        "    print(f'Processing {str(n)} started as no {str(i)}')\n",
        "    #URI_list_df  = pd.read_csv(f'{filelocation_google}/EuropeanaObjectURIlist/Creator/{n}')\n",
        "    URI_list_df  = pd.read_csv(f'EuropeanaObjectURIlist/Contributor/{n}')    \n",
        "    list_Europeana_proxy = (URI_list_df['Europeana_proxy']).to_list()\n",
        "    #list_Wikidata_URI = (URI_list_df['Wikidata_URI']).to_list()\n",
        "    #list_Europeana_proxy = list_Europeana_proxy[0:10]\n",
        "    #list_Wikidata_URI = list_Wikidata_URI[0:10]\n",
        "    #print(list_Europeana_proxy)\n",
        "    # Create directory for each iterated file\n",
        "    #folder_path = os.path.join(f'{filelocation_google}/EuropeanaObjectTurtle/test/', str(n))\n",
        "    folder_path = os.path.join(f'EuropeanaObjectTurtle/test/Contributor/', str(n))\n",
        "    os.mkdir(folder_path)\n",
        "    \n",
        "    #df = pd.DataFrame(columns=['Europeana URI', 'Success', 'Info'])\n",
        "\n",
        "    def singlefile_download(item):\n",
        "      print(f'Processing {str(item)} started')\n",
        "      #print(var2)\n",
        "      #print(list_DBpedia_URI[i])\n",
        "      headers = {\n",
        "        'Accept': 'text/turtle',\n",
        "        'Content-type': 'text/turtle'\n",
        "      }\n",
        "      turtle_europeana = europeana_http_request(item, headers)\n",
        "      # Check if turtle file was fetched\n",
        "      if turtle_europeana != None:\n",
        "        filename = item.replace('/', '_')\n",
        "        #filename = item.rsplit('/', 1)[1]\n",
        "        print(f'{filename} is fetched')\n",
        "        path_to_file = f'{folder_path}/{str(filename)}.ttl'\n",
        "        path = Path(path_to_file)\n",
        "        # Check if the (same) turtle file was already saved \n",
        "        if path.is_file():\n",
        "          print(f'The file {path_to_file} already exists, thus no need to save and download to local')\n",
        "        else:\n",
        "          # Download ttl to GoogleDrive\n",
        "          with open(path_to_file, 'w') as f:\n",
        "            f.write(turtle_europeana)\n",
        "          #success = 1\n",
        "          #message = ''\n",
        "          # Download ttl to local machine\n",
        "          #files.download(filelocation_google + 'EuropeanaObjectTurtle/test/' + str(filename) + '.ttl')\n",
        "          print(f'The file {path_to_file} does not exist, thus saved and download to local')\n",
        "      else:\n",
        "        pass\n",
        "        #success = 0\n",
        "        #message = str(turtle_europeana)\n",
        "        print(f'No Turtle can be fetched from {str(item)}')\n",
        "\n",
        "      #dicts = {'Europeana URI': path_to_file, 'Success': success, 'Info': message}\n",
        "      #df_dicts = pd.DataFrame([dicts])\n",
        "      #df = pd.concat([df, df_dicts], ignore_index=True)\n",
        "      \n",
        "      print(f'Processing {str(item)} completed')\n",
        "      #Time interval for a next query/iteration\n",
        "      #time.sleep(2)\n",
        "      print('-----------------')\n",
        "\n",
        "\n",
        "    # Time recording\n",
        "    t1 = time.perf_counter()\n",
        "    now1 = datetime.datetime.now()\n",
        "    print('==========================================')\n",
        "    print( f'{str(n)} is started {now1.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "    print('==========================================')\n",
        "    #Initiate max 40 threads (i.e. workers) \n",
        "    ex = futures.ThreadPoolExecutor(max_workers=2)\n",
        "    #print('{}: is starting work'.format(threading.current_thread().getName()))\n",
        "    #Start the threads with the map method\n",
        "    results = ex.map(singlefile_download, list_Europeana_proxy)\n",
        "    #print('{}: is waiting for the results'.format(threading.current_thread().getName()))\n",
        "    real_results = list(results)\n",
        "    #print('main: results: {}'.format(real_results))\n",
        "\n",
        "    t2 = time.perf_counter()\n",
        "    now2 = datetime.datetime.now()\n",
        "    duration_seconds = t2 - t1\n",
        "    duration = str(timedelta(seconds=duration_seconds))\n",
        "\n",
        "    nl = '\\n'\n",
        "    # Save the time recording log\n",
        "    with open(f'EuropeanaObjectTurtle/test/Contributor/' + str(n) + '_log_threading.txt', 'w') as f:\n",
        "      # print(f.write(inputfile + ' is completed in ' + duration + ' at ' +  now2.strftime(\"%Y-%m-%d %H:%M:%S\")) + '\\n')\n",
        "      print(f.write(f'{str(n)}: {nl} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Duration: {duration} = {duration_seconds} seconds {nl} ------- {nl}'))\n",
        "\n",
        "    print('==========================================')\n",
        "    #print(file + ' is completed in ' + duration + ' at ' +  now2.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "    print(f'{str(n)}: {nl} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Duration: {duration} = {duration_seconds} seconds')\n",
        "    print('==========================================')\n",
        "\n",
        "\n",
        "    # #### Multithreading below\n",
        "    # t1 = time.perf_counter()\n",
        "    # now1 = datetime.datetime.now()\n",
        "    # #Initiate max 40 threads (i.e. workers) \n",
        "    # ex = futures.ThreadPoolExecutor(max_workers=2)\n",
        "    # #ex = futures.ThreadPoolExecutor()\n",
        "    # print('{}: is starting work'.format(threading.current_thread().getName()))\n",
        "    # #Start the threads with the map method\n",
        "    # results = ex.map(singlefile_download, list_Europeana_proxy)\n",
        "    # #print('{}: is waiting for the results'.format(threading.current_thread().getName()))\n",
        "    # real_results = list(results)\n",
        "    # print('main: results: {}'.format(real_results))\n",
        "    # t2 = time.perf_counter()\n",
        "    # now2 = datetime.datetime.now()\n",
        "    # duration_seconds = round(t2 - t1)\n",
        "    # duration = str(timedelta(seconds=duration_seconds))\n",
        "    # nl = '\\n'\n",
        "    # # Save the time recording log\n",
        "    # with open(f'EuropeanaObjectTurtle/test/Creator/' + str(n) + '_log_threading.txt', 'w') as f:\n",
        "    #     f.write(f'THREADING: {nl} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Duration: {duration} = {duration_seconds} seconds')\n",
        "    # print(f'========================================================')\n",
        "    # print(f'THREADING {nl} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Duration: {duration} = {duration_seconds} seconds')\n",
        "    # print(f'========================================================')\n",
        "\n",
        "    # Idea to store results of metadata fetching in Dataframe per CSV file of Europeana URI list, but not successful so far\n",
        "    #df_csv = df.to_csv(f'{str(n)}.csv', index = False)\n",
        "    # Reset the dataframe for the next iteration\n",
        "    #df = df[0:0]\n",
        "    \n",
        "    # #Compare it to a serial run\n",
        "    # t1 = time.perf_counter()\n",
        "    # now1 = datetime.datetime.now()\n",
        "    # for item in list_Europeana_proxy:\n",
        "    #   singlefile_download(item)        \n",
        "    # t2 = time.perf_counter()\n",
        "    # now2 = datetime.datetime.now()\n",
        "    # duration_seconds = round(t2 - t1, 3)\n",
        "    # duration = str(timedelta(seconds=duration_seconds))\n",
        "    # # Save the time recording log\n",
        "    # with open(f'{folder_path}/log_serial.txt', 'w') as f:\n",
        "    #     f.write(f'SERIAL {nl} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Duration: {duration} = {duration_seconds} seconds')\n",
        "    # print(f'========================================================')\n",
        "    # print(f'SERIAL {nl} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Duration: {duration} = {duration_seconds} seconds')\n",
        "    # print(f'========================================================')\n",
        "\n",
        "    # # Save the time recording log\n",
        "    # with open(f'{folder_path}/log.txt', 'w') as f:\n",
        "    #     f.write(f'{str(n)} {nl} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Duration: {duration} = {duration_seconds} seconds')\n",
        "    # print(f'========================================================')\n",
        "    # print(f'{str(n)} {nl} Completed = {str(i +1)} files processed {nl} Start time  {now1.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Finish time {now2.strftime(\"%Y-%m-%d %H:%M:%S\")} {nl} Duration: {duration} = {duration_seconds} seconds')\n",
        "    # print(f'========================================================')\n",
        "\n",
        "    i = i + 1\n",
        "\n",
        "multiple_files_download(list_file)\n",
        "\n",
        "# Peformance benchmark CPU\n",
        "# 2 iterations with no sleep\n",
        "# 4 Threading 2.72, 2.65 seconds\n",
        "# Serial process 8.02, 7.56 seconds\n",
        "#\n",
        "# 2 iterations with 1 second sleep\n",
        "# 4 Threading 6.78, 5.29 seconds\n",
        "# Serial process 18.30, 18.00 seconds\n",
        "#\n",
        "# 2 iterations with 2 seconds sleep\n",
        "# 4 Threading 8.70, 8.31 seconds\n",
        "# Serial process 28.47, 27.92 seconds\n",
        "\n",
        "# Peformance benchmark GPU\n",
        "# 2 iterations with no sleep\n",
        "# 4 Threading 3.66, 3.38 seconds\n",
        "# Serial process 11.47, 10.85 seconds\n",
        "#\n",
        "# 2 iterations with 1 second sleep\n",
        "# 4 Threading 6.48, 6.55 seconds \n",
        "# Serial process 21.75, 20.34 seconds \n",
        "#\n",
        "# 2 iterations with 2 seconds sleep\n",
        "# 4 Threading 9.93, 9.16 seconds\n",
        "# Serial process 31.74, 30.87 seconds\n",
        "\n",
        "# Peformance benchmark TPU\n",
        "# 2 iterations with no sleep\n",
        "# 4 Threading 2.98, 2.22 seconds\n",
        "# Serial process 9.09, 8.15 seconds\n",
        "#\n",
        "# 2 iterations with 1 second sleep\n",
        "# 4 Threading 5.53, 5.27 seconds \n",
        "# Serial process 18.40, 17.32 seconds \n",
        "#\n",
        "# 2 iterations with 2 seconds sleep\n",
        "# 4 Threading 8.85, 8.16 seconds\n",
        "# Serial process 28.27, 29.33 seconds\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculated the number of downloaded files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# folder path to CSV files\n",
        "dir_path = 'EuropeanaObjectTurtle/test/Creator/'\n",
        "# Create an empty list\n",
        "list_file = []\n",
        "list_sub_file = []\n",
        "number_file_in_folder = []\n",
        "# Iterate in the folder\n",
        "for path in os.listdir(dir_path):\n",
        "    # check if current path is a folder\n",
        "    if os.path.isdir(os.path.join(dir_path, path)):\n",
        "        # Iterate in the subfolder\n",
        "        sub_dir_path = path\n",
        "        #print(dir_path + sub_dir_path)\n",
        "        \n",
        "        for sub_path in os.listdir(dir_path + sub_dir_path):\n",
        "            #print(sub_path)    \n",
        "            #check if current path is a file\n",
        "            if os.path.isfile(os.path.join(dir_path, sub_dir_path, sub_path)):\n",
        "                list_sub_file.append(sub_path)\n",
        "            else:\n",
        "                pass\n",
        "        \n",
        "        list_file.append(list_sub_file)\n",
        "\n",
        "        # length = len(list_sub_file)\n",
        "        # number_file_in_folder.append(length)    \n",
        "        # n = n + 1\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "#list_file.sort()\n",
        "print(list_file)\n",
        "print('--------')\n",
        "print(number_file_in_folder)\n",
        "print('--------')\n",
        "#list_sub_file.sort()\n",
        "print(len(number_file_in_folder))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Vli9GyRiia"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJO6KUnYEDg1"
      },
      "outputs": [],
      "source": [
        "while True:pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA2qUbIZ5y0Q"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "056af9df3335714b92606d06f1f2fa8228684766ba1a5b77570ea43fb51e6c01"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
